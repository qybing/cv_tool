import os
import hashlib
import time
import shutil
from concurrent.futures import ThreadPoolExecutor
from collections import defaultdict
import imghdr
import logging


class ImageDeduplicator:
    def __init__(self, src_dir, dup_dir, min_file_size=4096, fast_mode=True, keep_mode="first", max_workers=None):
        """
        åˆå§‹åŒ–å›¾ç‰‡å»é‡å™¨

        :param src_dir: æºå›¾ç‰‡ç›®å½•è·¯å¾„
        :param dup_dir: é‡å¤æ–‡ä»¶å­˜æ”¾ç›®å½•è·¯å¾„
        :param min_file_size: æœ€å°å¤„ç†çš„æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰ï¼Œé»˜è®¤ä¸º4KB
        :param fast_mode: æ˜¯å¦ä½¿ç”¨å¿«é€Ÿæ¨¡å¼ï¼ˆé‡‡æ ·å“ˆå¸Œï¼‰ï¼Œé»˜è®¤ä¸ºTrue
        :param keep_mode: ä¿ç•™æ¨¡å¼ï¼Œå¯é€‰ "first"ï¼ˆä¿ç•™ç¬¬ä¸€ä¸ªï¼‰ã€"last"ï¼ˆä¿ç•™æœ€åä¸€ä¸ªï¼‰ã€"oldest"ï¼ˆæœ€æ—§æ–‡ä»¶ï¼‰
        :param max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°ï¼ŒNoneåˆ™ä½¿ç”¨CPUæ ¸å¿ƒæ•°*2
        """
        self.src_dir = os.path.abspath(src_dir)
        self.dup_dir = os.path.abspath(dup_dir)
        self.min_file_size = min_file_size
        self.fast_mode = fast_mode
        self.keep_mode = keep_mode
        self.max_workers = max_workers or (os.cpu_count() * 2)

        # æ—¥å¿—è®°å½•
        self.total_images = 0
        self.duplicate_groups = 0
        self.files_moved = 0
        self.space_freed = 0
        self.processing_time = 0

        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler("image_deduplication.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger()

        # éªŒè¯é…ç½®
        if not os.path.isdir(self.src_dir):
            raise ValueError(f"æºç›®å½•ä¸å­˜åœ¨: {self.src_dir}")
        os.makedirs(self.dup_dir, exist_ok=True)

        # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
        self.supported_formats = {"jpg", "jpeg", "png", "gif", "bmp", "tiff", "webp"}

    def is_valid_image(self, file_path):
        """éªŒè¯æ–‡ä»¶æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å›¾ç‰‡æ ¼å¼å¹¶æ»¡è¶³æœ€å°å¤§å°è¦æ±‚"""
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            if os.path.getsize(file_path) < self.min_file_size:
                return False

            # æ£€æŸ¥æ–‡ä»¶å¤´
            with open(file_path, 'rb') as f:
                header = f.read(16)
                if not header:
                    return False

                # ä½¿ç”¨imghdræ£€æµ‹æ ¼å¼
                file_format = imghdr.what(None, h=header)
                return file_format and file_format.lower() in self.supported_formats
        except (OSError, IOError):
            return False

    def calculate_file_hash(self, file_path):
        """è®¡ç®—æ–‡ä»¶çš„å“ˆå¸Œå€¼"""
        hasher = hashlib.blake2b(digest_size=16)
        file_size = os.path.getsize(file_path)

        # å¯¹å¤§æ–‡ä»¶ä½¿ç”¨é‡‡æ ·ç­–ç•¥
        if self.fast_mode and file_size > 1024 * 1024:  # >1MBçš„æ–‡ä»¶ä½¿ç”¨å¿«é€Ÿé‡‡æ ·
            with open(file_path, 'rb') as f:
                # æ–‡ä»¶å¤´
                hasher.update(f.read(4096))

                # æ–‡ä»¶ä¸­æ®µ
                if file_size > 8192:
                    f.seek(file_size // 2 - 2048)
                    hasher.update(f.read(4096))

                # æ–‡ä»¶å°¾
                if file_size > 12288:
                    f.seek(-4096, 2)
                    hasher.update(f.read(4096))
        else:
            # å®Œæ•´æ–‡ä»¶å“ˆå¸Œ
            with open(file_path, 'rb') as f:
                # ä½¿ç”¨å†…å­˜è§†å›¾ä¼˜åŒ–æ€§èƒ½
                while chunk := f.read(65536):
                    hasher.update(chunk)

        return hasher.hexdigest()

    def process_image(self, file_info):
        """å¤„ç†å•ä¸ªå›¾åƒæ–‡ä»¶å¹¶è®¡ç®—å“ˆå¸Œå€¼"""
        file_path, file_size = file_info
        try:
            if self.is_valid_image(file_path):
                return self.calculate_file_hash(file_path), file_path, file_size
        except Exception as e:
            self.logger.error(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}")
        return None, None, None

    def find_duplicates(self):
        """æŸ¥æ‰¾ç›®å½•ä¸­çš„é‡å¤å›¾ç‰‡"""
        start_time = time.time()
        self.logger.info(f"ğŸ” æ‰«ææºç›®å½•: {self.src_dir}")
        self.logger.info(f"âš¡ ä½¿ç”¨ {'å¿«é€Ÿ' if self.fast_mode else 'å®Œæ•´'} æ¨¡å¼æ£€æµ‹é‡å¤")
        self.logger.info(f"ğŸ“ æœ€å°å¤„ç†å¤§å°: {self.min_file_size} å­—èŠ‚")

        # æ”¶é›†å€™é€‰å›¾ç‰‡æ–‡ä»¶ï¼ˆæ–‡ä»¶è·¯å¾„ + æ–‡ä»¶å¤§å°ï¼‰
        candidate_images = []
        for root, _, files in os.walk(self.src_dir):
            for filename in files:
                file_path = os.path.join(root, filename)
                if not os.path.isfile(file_path):
                    continue

                try:
                    file_size = os.path.getsize(file_path)
                    # é¦–å…ˆæ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦è¾¾åˆ°æœ€å°è¦æ±‚
                    if file_size >= self.min_file_size:
                        candidate_images.append((file_path, file_size))
                except OSError as e:
                    self.logger.warning(f"æ— æ³•è·å–æ–‡ä»¶å¤§å°: {file_path}: {str(e)}")

        self.total_images = len(candidate_images)
        if not candidate_images:
            self.logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å›¾ç‰‡æ–‡ä»¶")
            return {}

        self.logger.info(f"ğŸ“Š å‘ç° {self.total_images} ä¸ªå€™é€‰å›¾ç‰‡æ–‡ä»¶")

        # å¹¶è¡Œå¤„ç†æ‰€æœ‰å€™é€‰å›¾ç‰‡
        hash_map = defaultdict(list)
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(self.process_image, file_info) for file_info in candidate_images]

            for i, future in enumerate(futures):
                file_hash, file_path, file_size = future.result()
                if file_hash and file_path and file_size:
                    hash_map[file_hash].append((file_path, file_size))

                # è¿›åº¦æŠ¥å‘Š
                if i % 500 == 0 or i == len(candidate_images) - 1:
                    processed = i + 1
                    self.logger.info(f"ğŸ”„ å¤„ç†ä¸­: {processed}/{self.total_images} ({processed / self.total_images:.1%})")

        # è¯†åˆ«é‡å¤é¡¹ï¼ˆå“ˆå¸Œå€¼ç›¸åŒçš„æ–‡ä»¶ï¼‰
        duplicates = {}
        for file_hash, files in hash_map.items():
            if len(files) > 1:
                # æŒ‰è·¯å¾„æ’åºç¡®ä¿å¯é¢„æµ‹æ€§
                duplicates[file_hash] = [file_path for file_path, _ in sorted(files)]
                self.duplicate_groups += 1

        self.processing_time = time.time() - start_time
        self.logger.info(f"âœ… å‘ç° {self.duplicate_groups} ç»„é‡å¤å›¾ç‰‡")
        self.logger.info(f"â± æ£€æµ‹è€—æ—¶: {self.processing_time:.2f}ç§’")

        return duplicates

    def move_duplicates(self, duplicates):
        """ç§»åŠ¨é‡å¤æ–‡ä»¶åˆ°æŒ‡å®šç›®å½•"""
        if not duplicates:
            self.logger.info("ğŸ‰ æœªå‘ç°é‡å¤å›¾ç‰‡")
            return

        self.logger.info(f"ğŸšš å¼€å§‹ç§»åŠ¨é‡å¤æ–‡ä»¶åˆ°: {self.dup_dir}")
        start_time = time.time()
        moved_count = 0
        freed_space = 0

        # ä¸ºæ¯ä¸ªå“ˆå¸Œç»„åˆ›å»ºç›®æ ‡å­ç›®å½•
        for file_hash, files in duplicates.items():
            # ç¡®å®šè¦ä¿ç•™çš„æ–‡ä»¶ï¼ˆä¿æŒåŸä½ç½®ï¼‰
            if self.keep_mode == "first":
                keep_file = files[0]
                move_files = files[1:]
            elif self.keep_mode == "last":
                keep_file = files[-1]
                move_files = files[:-1]
            else:  # oldest æ¨¡å¼ï¼ˆæœ€åä¿®æ”¹æ—¶é—´æœ€æ—©ï¼‰
                files_with_mtime = [(f, os.path.getmtime(f)) for f in files]
                files_with_mtime.sort(key=lambda x: x[1])
                keep_file = files_with_mtime[0][0]
                move_files = [f[0] for f in files_with_mtime[1:]]

            # ä¸ºå½“å‰å“ˆå¸Œç»„åˆ›å»ºç›®æ ‡å­ç›®å½•
            group_dir = os.path.join(self.dup_dir, f"group_{file_hash[:8]}")
            os.makedirs(group_dir, exist_ok=True)

            # ç§»åŠ¨é‡å¤æ–‡ä»¶
            for file_path in move_files:
                try:
                    # ä¿æŒç›¸å¯¹è·¯å¾„ç»“æ„
                    rel_path = os.path.relpath(file_path, self.src_dir)
                    dest_path = os.path.join(group_dir, os.path.basename(rel_path))

                    # å¤„ç†æ–‡ä»¶åå†²çª
                    counter = 1
                    base, ext = os.path.splitext(dest_path)
                    while os.path.exists(dest_path):
                        dest_path = f"{base}_{counter}{ext}"
                        counter += 1

                    # è·å–æ–‡ä»¶å¤§å°å¹¶ç§»åŠ¨
                    file_size = os.path.getsize(file_path)
                    shutil.move(file_path, dest_path)

                    freed_space += file_size
                    moved_count += 1
                    self.logger.debug(f"ç§»åŠ¨: {file_path} â†’ {dest_path} ({file_size / 1024:.2f} KB)")

                    # å®šæœŸæŠ¥å‘Šè¿›åº¦
                    if moved_count % 50 == 0:
                        self.logger.info(f"ğŸ“¦ å·²ç§»åŠ¨ {moved_count} ä¸ªæ–‡ä»¶ | "
                                         f"é‡Šæ”¾ç©ºé—´: {freed_space / 1024 / 1024:.2f} MB")
                except Exception as e:
                    self.logger.error(f"ç§»åŠ¨å¤±è´¥ {file_path}: {str(e)}")

        # æ›´æ–°ç»Ÿè®¡æ•°æ®
        self.files_moved = moved_count
        self.space_freed = freed_space
        move_time = time.time() - start_time

        self.logger.info(f"ğŸ“¦ ç§»åŠ¨å®Œæˆ: {moved_count} æ–‡ä»¶")
        self.logger.info(f"ğŸ’¾ é‡Šæ”¾ç©ºé—´: {freed_space:,} å­—èŠ‚ ({freed_space / 1024 / 1024:.2f} MB)")
        self.logger.info(f"â± ç§»åŠ¨è€—æ—¶: {move_time:.2f}ç§’")

    def generate_report(self):
        """ç”Ÿæˆå¤„ç†ç»“æœæŠ¥å‘Š"""
        report = [
            "\n" + "=" * 60,
            "ğŸ“Š å›¾ç‰‡å»é‡æŠ¥å‘Š",
            "=" * 60,
            f"ğŸ”¸ æºç›®å½•: {self.src_dir}",
            f"ğŸ”¸ é‡å¤æ–‡ä»¶ç›®å½•: {self.dup_dir}",
            f"ğŸ”¸ æ‰«æå›¾ç‰‡æ€»æ•°: {self.total_images}",
            f"ğŸ”¸ å‘ç°é‡å¤ç»„: {self.duplicate_groups}",
            f"ğŸ”¸ ç§»åŠ¨æ–‡ä»¶æ•°: {self.files_moved}",
            f"ğŸ”¸ é‡Šæ”¾ç©ºé—´: {self.space_freed:,} å­—èŠ‚ ({self.space_freed / 1024 / 1024:.2f} MB)",
            f"ğŸ”¸ æ€»å¤„ç†æ—¶é—´: {self.processing_time:.2f} ç§’",
            "",
            "ğŸ’¡ æç¤º: é‡å¤æ–‡ä»¶å·²æŒ‰å“ˆå¸Œç»„åˆ†ç»„å­˜æ”¾ï¼Œè¯·æ£€æŸ¥ååˆ é™¤",
            "=" * 60
        ]

        for line in report:
            self.logger.info(line)

        # åŒæ—¶ä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶
        with open("deduplication_report.txt", "w") as f:
            f.write("\n".join(line.replace("ğŸ”¸ ", "").replace("ğŸ’¡ ", "").replace("ğŸ“Š ", "").replace("=" * 60, "=" * 50)))


if __name__ == "__main__":
    # é…ç½®å‚æ•°
    SOURCE_DIR = "/home/jovan/dataset/shandong/smog/train0707/images_duplicate"  # ä¿®æ”¹ä¸ºæ‚¨çš„æºç›®å½•
    DUPLICATE_DIR = "/home/jovan/dataset/shandong/smog/train0707/duplicates"  # ä¿®æ”¹ä¸ºæ‚¨çš„ç›®æ ‡ç›®å½•
    MIN_FILE_SIZE = 100  # å­—èŠ‚
    FAST_MODE = True
    KEEP_MODE = "first"  # "first", "last" or "oldest"

    # æ‰§è¡Œå»é‡
    try:
        deduper = ImageDeduplicator(
            src_dir=SOURCE_DIR,
            dup_dir=DUPLICATE_DIR,
            min_file_size=MIN_FILE_SIZE,
            fast_mode=FAST_MODE,
            keep_mode=KEEP_MODE
        )

        duplicates = deduper.find_duplicates()
        deduper.move_duplicates(duplicates)
        deduper.generate_report()
    except Exception as e:
        logging.error(f"âŒ ç¨‹åºé”™è¯¯: {str(e)}")
        raise
